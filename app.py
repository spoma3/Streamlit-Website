import streamlit as st
import pandas as pd
st.set_page_config(page_title = "CS 4641 Group #47 Team Proposal", page_icon = ":potted_plant:", layout = "wide")
# st.subheader("Want to watch our unlisted Youtube video on this topic go [here](https://youtu.be/Uc_2V9tAKqM)")
st.write("---")
# --- HEADER SECTION ----
with st.container():
    left_column, right_column = st.columns(2)
    with right_column:
        st.image("SoyBeanImage.jpg", width = 450, caption="Soybean Farm")
    with left_column:
        st.subheader("Introduction/Background")
        st.title("Literature Review")
        st.write("Our topic of choice is Advanced Soybean Agriculture, more specifically predicting soybean yield based on salicylic acid treatment, genotype, and water stress. Pre–existing research has found that salicylic acid treatments of 100-200 parts per million result in significantly higher soybean yields [1]. Furthermore, genotypes with larger roots systems tend to have water retention rates, which also results in higher yields [2]. Higher levels of water stress are often associated with lower plant height and weight, leaf size, and overall seed yield [3].")
        st.title("Dataset Description")
        st.write("Our dataset is “Advanced Soybean Agricultural Dataset” from Kaggle. This dataset consists of parameters including genotype, salicylic acid treatment, and water stress. There are 6 different genotypes, 3 levels of salicylic acid treatment: 250 mg, 450 mg, and 0 mg, and 2 levels of water stress: 5% and 70%. Furthermore, the dataset shows various quantitative metrics based on parameters such as plant height, biological weight, number of pods, and many others.")
        st.title("Dataset Link")
        st.write("[Dataset Link is here](https://www.kaggle.com/datasets/wisam1985/advanced-soybean-agricultural-dataset-2025)")
        st.subheader("Problem Definition")
        st.title("Problem")
        st.write("Using current agricultural practices, it is difficult to predict and optimize soybean yield and health. This leads to inconsistencies that can potentially cause supply chain disruptions, price instability, inefficient allocation of resources, and an overall decline in quality and output. With the soybean industry already fraught with supply chain bottlenecks, it is imperative to determine the most efficient method to yield the highest quantity and quality of soybeans in order to meet rising global demand.")
        st.subheader("Methods")
        st.title("Normalization of Data")
        st.write("The data preprocessing methods chosen for this project aim to enhance the accuracy of the machine learning algortihm while reducing the risk of overfitting. So far, normalization and z-score standardization have been implemented. Normalization was applied to prevent certain features from disproportionately influencing model weights and loss functions. This issue arises when some features have naturally larger values, amplifying the impact of incorrect estimates. Normalization was implemented with the following equation:  norm_val = (vals – min(val)) / (max(val) - min(val)).")
        st.image("Normalization Function.png")
        st.title("Random Forest Regression Model")
        st.write("After min-max scaling four continuous agronomic features (Water Val, C Val, Chlorophyll A663, Chlorophyll A645) and six one-hot genotype flags, we trained a supervised Random Forest Regressor. The cleaned data set (about 680 plots, 20 % train / 80 % test) was first explored with a coarse grid sweep of n_estimators from 10 to 450 at max_depth = 5. Because error continued to drop beyond 450 trees, we extended the sweep; the best configuration—460 trees, depth = 5—achieved R² = 0.89, MAE ≈ 75 kg ha⁻¹, and RMSE ≈ 110 kg ha⁻¹ on the hold-out set. For interpretation, the six genotype importances from feature_importances_ were aggregated into a single Genotype score. The consolidated ranking was Chlorophyll A663 (0.34), Genotype (0.27), Water Val (0.17), C Val (0.13), and Chlorophyll A645 (0.09), showing that photosynthetic capacity and genetic line dominate yield prediction, while moisture and carbon allocation provide secondary signal.")
        st.image("Feature Importance Graph.png")
        st.title("Outlier Removal")
        st.write("Before any modeling we removed 47 observations whose yield had an absolute z-score greater than 3. Although this filter was originally added to stabilize the neural-network experiments, we kept it for the Random Forest so every algorithm trained and was evaluated on the same clean data. Eliminating these extreme-yield plots prevented distortions in split thresholds and narrowed MAE and RMSE by roughly eight percent.")
        st.image("Outlier Removal.png")
        st.title("Neural Network")
        st.write("Finally, a supervised learning Neural Network was implemented to capture complex variable interactions and improve predictive accuracy, setting the foundation for constructing the Bayesian Neural Network.")
        st.image("Neural Network.png")
        st.write("1. Achieve at least 90% accuracy in predicting protein soybean yield (protein % * biological weight) or other plant characteristics.")
        st.write("2. Help identify optimal salicylic acid treatment, genotype, water stress and other characteristics of soybean plants for optimal protein yields or other types of plant yields.")
        st.write("PCA")
        st.write("After normalization and the random forest test the next step is to perform PCA on the data to reduce the number of features. From our random forest test we saw that about 80% of the feature importance was concentrated on 3 features so we decided to reduce the number of features used in our model down to only 3 features. This was done to prevent overfitting in the model data when the data has too many features to depend on leading to it not being able to properly generalize trends in the data as there are too many dimensions in the dataset caused by many unimportant features with low variance. However, this didn’t work because the model ended up having a greatly reduced accuracy because compressing the 5 dimensional points into 3 dimensions greatly reduced the BNNs ability to create accurate regression models to separate the data so we removed them because the improvements in computational efficiency were outweighed by the reductions in the model’s accuracy.")
        st.write("BNN")
        st.write("After the PCA a BNN model was performed with the goal of determining the certainty of predicting specific results to be used as a way to inform farmers on how much to trust the model overall.")
        st.title("Visualization")
        st.write("We performed two experiments using different machine learning models to predict the seed yield per unit area (SYUA) based on a reduced feature set. Below are the visualizations representing the performance of the models.")
        st.title("Random Forest Regressor Model")
        st.write("We evaluated the Random Forest Regressor (RFR) model by varying the number of trees and observing the change in the R² error. The following graph shows how the R² error changed with the number of trees:")
        st.image("RFR_R^2.png")
        st.caption("f**{Figure 1.}** As shown in the plot, the R² error levels out as the number of trees passes 460, indicating that increasing the number of trees past 460 doesn’t significantly improve the model's performance. To prevent overfitting the model had its training and testing split set to 20 and 80 respectively to prevent the model from memorizing the data.")
        st.title("Neural Network Model")
        st.write("The loss of the neural network model to its testing data during training was visualized over epochs. The following graph shows the loss curve:")
        st.image("NN_Loss.png")
        st.caption("f**{Figure 2.}** This plot demonstrates that the model steadily decreased its loss over the 4 epochs, which indicates that the training process was effective and the model was able to learn meaningful patterns in the data.")
        st.title("Bayesian Neural Network")
        st.title("Y_train quantile")
        st.image("y_train_dist.png")
        st.caption("f**{Figure 3.}** This image shows the smoothing effect of the quantile distribution function which was implemented to give y_train and y_test a more gaussian curve to make it easier for the bayesian neural network to process and learn from the data.")
        st.title("Bayesian Neural Network Model")
        st.image("BNN Loss.png")
        st.caption("f**{Figure 4.}** The loss of the bayesian neural network model to the training data is shown here. A key thing to note is that the loss trends downwards over time except for the last 10 epochs where it trends up highlighting that the model’s overfitting protections work well")
        st.image("BNN Uncertainty.png")
        st.caption("f**{Figure 5.}** Here is the uncertainty as you can see it seems to randomly fluctuate and never significantly change in the model from what we found.")
        st.title("Quantitative Metrics")
        st.title("Random Forest Regressor (RFR) Model")
        st.write("After selecting the optimal number of trees (410), we evaluated the Random Forest model on the test set, achieving an R² score of 0.899. This indicates that the model can explain 89.9% of the variance in the seed yield per unit area. Such a high R² score suggests that the model has performed exceptionally well on the test data.")
        st.title("Neural Network Model")
        st.write("The neural network achieved a R² score of 0.9399, which is also a strong result, indicating the model is able to predict the seed yield with good accuracy. The mean squared error (MSE) for the model is 0.0029, suggesting the predictions are close to the actual values.")
        st.title("Bayesian Neural Network Model")
        st.write("The Bayesian neural network, trained on the same outlier-filtered and scaled dataset, produced an MAE of 0.04 highlighting its ability to properly fit the data though not better than the Neural Networks loss of about 0.03.")
        st.write("Analysis of Model Performance")
        st.title("Random Forest Regressor (RFR)")
        st.write("The RFR model performed very well, with a very high R² score of 0.88 on its test data. This suggests that the ensemble approach of Random Forest, using multiple decision trees, is well-suited to our problem. The model's performance could be attributed to its ability to model non-linear relationships and handle feature interactions. The RFR model had its overfitting fixed by introducing max_depths and decreasing the size of the training data so that it isn’t the majority of the testing data to prevent the RFR model from simply memorizing the data. The feature importance could also be investigated further to see how each input feature contributes to the prediction.")
        st.title("Neural Network")
        st.write("The neural network's performance is also commendable, with an R² score of 0.92 on the testing data. The neural network model had its overfitting fixed by simplifying the neural network model with fewer layers, increasing the learning rate from 0.001 to 0.005, the number of epochs was reduced from 8 to 4 epochs. But most importantly the percentage of the data allotted to training was decreased from 80% to 20%. These choices were done so that the program couldn’t memorize the data and its outputs but instead had to memorize the general trend by not being able to see as much data with each epoch and by not being able to train its weights to the training data instead of the broader data trends. A longer training period with more epochs could potentially lead to improved results since the program currently isn’t looking at as much data as it could and may benefit from more iterations to see more data to get a better grasp of the overall trends since the risk of overfitting is minimal with a batch size of only 64 in a dataset of 55,540 data points.") 
        st.write("Bayesian Neural Network")
        st.write("Now, we implemented a Bayesian Neural Network (BNN) using TensorFlow and TensorFlow Probability, incorporating probabilistic reasoning and uncertainty estimation. There was an extra step of data preprocessing for this model by using Quantile Transformer to fit the data into a Gaussian distribution. The model used Bayesian Linear layers to apply Bayesian inference to the weights, allowing for more robust predictions. We trained the BNN with the RMSprop optimizer,and tracked the accuracy using the mean absolute error loss and tracked the standard deviations in the predictions to assess the impact of uncertainty in the model's predictions. We chose Mean Absolute Error as our loss function, as it works well with uncertainty estimation and is more interpretable than MSE or R2. Furthermore, to prevent overfitting, we implemented the feature to stop if the Mean Absolute Error score does not improve after 10 epochs. The maximum number of epochs that our BNN will run for is 100. The performance of our BNN was commendable with the MAE score measured to be 0.04, which is excellent by conventional standards. While the BNN’s score was comparatively lower than that of the standard neural network, the trade-off was acceptable because of the BNN’s ability to provide confidence-aware predictions. However, one downside of our current BNN model was that it had a varying and unsteady level of uncertainty.")
        st.title("Next Steps")
        st.write("We believe that with further development, our BNN implementation could potentially perform better compared to the standard neural network counterpart in accuracy and uncertainty estimation. This would include adding a variable learning rate with a scheduler to start learning quickly, and gradually slowing down as the epochs continue for fine tuning. There could also be better analysis of uncertainty by summing together the epistemic and aleatoric uncertainties with epistemic uncertainty being the uncertainty that we could reduce and further improve on whereas aleatoric uncertainty is just a natural phenomenon in the data capture process.")
        st.subheader("Contributions Table")
        st.image("Contributions_Table.png", width = 300, caption = "Contributions")
        st.subheader("Gantt Chart")
        st.image("Gantt_Chart.png", width = 2500, caption = "Gantt Chart")
        st.subheader("References")
        st.title("References")
        st.write("[1]P. Kuchlan and M.K. Kuchlan, “Effect of Salicylic Acid on Plant Physiological and Yield Traits of Soybean,” Legume Research - An International Journal, no. Of, Mar. 2021, doi: https://doi.org/10.18805/lr-4527.")
        st.write("[2]Nisarga Kodadinne Narayana, Chathurika Wijewardana, F. A. Alsajri, K. Raja Reddy, S. R. Stetina, and Raju Bheemanahalli, “Resilience of soybean genotypes to drought stress during the early vegetative stage,” Scientific Reports, vol. 14, no. 1, Jul. 2024, doi: https://doi.org/10.1038/s41598-024-67930-w.")
        st.write("[3]M. S. Hossain et al., “Differential Drought Responses of Soybean Genotypes in Relation to Photosynthesis and Growth-Yield Attributes,” Plants, vol. 13, no. 19, pp. 2765–2765, Oct. 2024, doi: https://doi.org/10.3390/plants13192765.")
        st.write("[4] S. Jaiswal, “What is normalization in machine learning? A comprehensive guide to data rescaling,” DataCamp, https://www.datacamp.com/tutorial/normalization-in-machine-learning (accessed Feb. 20, 2025).")
        st.write("[5] “Normalize,” scikit, https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html (accessed Feb. 20, 2025).")
        st.write("[6] “Primary supervised learning algorithms used in Machine Learning: Exxact blog,” Exxact, https://www.exxactcorp.com/blog/Deep-Learning/primary-supervised-learning-algorithms-used-in-machine-learning (accessed Feb. 20, 2025).")
        st.write("[7] “Randomforestregressor,” scikit, https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html (accessed Feb. 20, 2025).")
        st.write("[8] “What is Random Forest?,” IBM, https://www.ibm.com/think/topics/random-forest (accessed Feb. 20, 2025).")
        st.write("[9] L. Hardester, “Explained: Neural networks,” MIT News | Massachusetts Institute of Technology, https://news.mit.edu/2017/explained-neural-networks-deep-learning-0414 (accessed Feb. 20, 2025).")
        st.write("[10] “4.2. permutation feature importance,” scikit, https://scikit-learn.org/stable/modules/permutation_importance.html (accessed Feb. 20, 2025).")
        st.write("[11] J. Q. Yu, E. Creager, D. Duvenaud, and J. Bettencourt, Bayesian Neural Networks, https://www.cs.toronto.edu/~duvenaud/distill_bayes_net/public/ (accessed Feb. 20, 2025).")
        st.write("[12] L. Sturlaugson, “Principal component analysis preprocessing with Bayesian networks ...,” Principal Component Analysis Preprocessing with Bayesian Networks for Battery Capacity Estimation, https://www.cs.montana.edu/sheppard/pubs/i2mtc-2013.pdf (accessed Feb. 21, 2025).")
